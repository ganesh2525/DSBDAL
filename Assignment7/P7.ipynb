{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "973cb77f-d310-4553-be88-e90c79cc826c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in d:\\programfiles\\python3810\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in d:\\programfiles\\python3810\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in d:\\programfiles\\python3810\\lib\\site-packages (from nltk) (1.4.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\programfiles\\python3810\\lib\\site-packages (from nltk) (2024.4.16)\n",
      "Requirement already satisfied: tqdm in d:\\programfiles\\python3810\\lib\\site-packages (from nltk) (4.66.2)\n",
      "Requirement already satisfied: colorama in d:\\programfiles\\python3810\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05bcb446-c84b-4a8e-9e87-d64bad6a0ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Ganesh\n",
      "[nltk_data]     Lokhande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Ganesh\n",
      "[nltk_data]     Lokhande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Ganesh\n",
      "[nltk_data]     Lokhande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Ganesh Lokhande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1e6da6b-cb83-4b03-9db3-32bab67a503c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sachin', 'was', 'the', 'goat', 'of', 'the', 'previous', 'generation', '.', 'Virat', 'is', 'the', 'GOAT', 'of', 'this', 'generation', '.', 'No', 'will', 'be', 'the', 'GOAT', 'of', 'next', 'generation', '.']\n",
      "['Sachin was the goat of the previous generation.', 'Virat is the GOAT of this generation.', 'No will be the GOAT of next generation.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, sent_tokenize\n",
    "corpus = \"Sachin was the goat of the previous generation. Virat is the GOAT of this generation. No will be the GOAT of next generation.\"\n",
    "print(word_tokenize(corpus))\n",
    "print(sent_tokenize(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8816ee05-8aa9-456b-85a6-1506a3aa047b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Sachin', 'NNP'), ('was', 'VBD'), ('the', 'DT'), ('goat', 'NN'), ('of', 'IN'), ('the', 'DT'), ('previous', 'JJ'), ('generation', 'NN'), ('.', '.'), ('Virat', 'NNP'), ('is', 'VBZ'), ('the', 'DT'), ('GOAT', 'NNP'), ('of', 'IN'), ('this', 'DT'), ('generation', 'NN'), ('.', '.'), ('No', 'DT'), ('will', 'MD'), ('be', 'VB'), ('the', 'DT'), ('GOAT', 'NNP'), ('of', 'IN'), ('next', 'JJ'), ('generation', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "tokens = word_tokenize(corpus)\n",
    "print(pos_tag(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a7b8f10-3a67-46a2-a962-82f44fc0016d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sachin', 'goat', 'previous', 'generation', '.', 'Virat', 'GOAT', 'generation', '.', 'No', 'GOAT', 'next', 'generation', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "tokens = word_tokenize(corpus)\n",
    "stop_words = set(stopwords.words(\"English\"))\n",
    "cleaned_tokens = []\n",
    "for token in tokens:\n",
    "    if(token not in stop_words):\n",
    "        cleaned_tokens.append(token)\n",
    "print(cleaned_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5e4e707-e7c3-40ed-a072-5a6d0b8c6c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sachin', 'goat', 'previou', 'gener', '.', 'virat', 'goat', 'gener', '.', 'no', 'goat', 'next', 'gener', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = []\n",
    "for token in cleaned_tokens:\n",
    "    stemmed = stemmer.stem(token)\n",
    "    stemmed_tokens.append(stemmed)\n",
    "print(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9c335fb-7790-49c1-96de-bad0c41e1c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sachin', 'goat', 'previous', 'generation', '.', 'Virat', 'GOAT', 'generation', '.', 'No', 'GOAT', 'next', 'generation', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = []\n",
    "for token in cleaned_tokens:\n",
    "    lemmatized = lemmatizer.lemmatize(token)\n",
    "    lemmatized_tokens.append(lemmatized)\n",
    "print(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4206355c-ce95-4a8d-b6b5-827c73cd0663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sachin': 9, 'was': 13, 'the': 10, 'goat': 2, 'of': 6, 'previous': 8, 'generation': 1, 'virat': 12, 'is': 3, 'this': 11, 'no': 5, 'one': 7, 'will': 14, 'be': 0, 'next': 4}\n",
      "  (0, 13)\t0.47698559825514686\n",
      "  (0, 10)\t0.28171537816186876\n",
      "  (0, 9)\t0.47698559825514686\n",
      "  (0, 8)\t0.47698559825514686\n",
      "  (0, 6)\t0.28171537816186876\n",
      "  (0, 2)\t0.28171537816186876\n",
      "  (0, 1)\t0.28171537816186876\n",
      "  (1, 12)\t0.47698559825514686\n",
      "  (1, 11)\t0.47698559825514686\n",
      "  (1, 10)\t0.28171537816186876\n",
      "  (1, 6)\t0.28171537816186876\n",
      "  (1, 3)\t0.47698559825514686\n",
      "  (1, 2)\t0.28171537816186876\n",
      "  (1, 1)\t0.28171537816186876\n",
      "  (2, 14)\t0.3954296357619344\n",
      "  (2, 10)\t0.2335471129161735\n",
      "  (2, 7)\t0.3954296357619344\n",
      "  (2, 6)\t0.2335471129161735\n",
      "  (2, 5)\t0.3954296357619344\n",
      "  (2, 4)\t0.3954296357619344\n",
      "  (2, 2)\t0.2335471129161735\n",
      "  (2, 1)\t0.2335471129161735\n",
      "  (2, 0)\t0.3954296357619344\n",
      "['be' 'generation' 'goat' 'is' 'next' 'no' 'of' 'one' 'previous' 'sachin'\n",
      " 'the' 'this' 'virat' 'was' 'will']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = [\n",
    "    \"Sachin was the GOAT of previous generation\",\n",
    "    \"Virat is the GOAT of this generation\",\n",
    "    \"No one will be the GOAT of next generation\"\n",
    "]\n",
    "vectorizer = TfidfVectorizer()\n",
    "print(vectorizer.fit(corpus).vocabulary_)\n",
    "print(vectorizer.transform(corpus))\n",
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "17d9be26-c453-4286-b175-fd7be010b7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def TF(doc,term):\n",
    "    words = doc.split()\n",
    "    term_count = words.count(term)\n",
    "    total_len = len(words)\n",
    "    tf = term_count/total_len\n",
    "    return tf\n",
    "\n",
    "def IDF(docs,term):\n",
    "    doc_count = sum(1 for doc in docs if term in doc)\n",
    "    total_docs = len(docs)\n",
    "    idf = math.log( total_docs/(1+doc_count))\n",
    "    return idf    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "18c4d4de-062c-4009-816e-8d0b362c21bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF: 0.2\n",
      "IDF: 0.28768207245178085\n",
      "TF-IDF: 0.05753641449035617\n"
     ]
    }
   ],
   "source": [
    "documents = [\n",
    "    \"This is the first document\",\n",
    "    \"This document is the second document\",\n",
    "    \"And this is third one\",\n",
    "    \"Is this first document ?\"\n",
    "]\n",
    "\n",
    "term = \"This\"\n",
    "tf = TF(documents[0],term)\n",
    "idf = IDF(documents,term)\n",
    "\n",
    "print(\"TF:\",tf)\n",
    "print(\"IDF:\",idf)\n",
    "print(\"TF-IDF:\",tf*idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abcde01-4598-4c53-803f-e5dde554764a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
